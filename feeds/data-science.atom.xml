<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Random Reasons &amp; Reflections - Data Science</title><link href="https://ojon.github.io/" rel="alternate"></link><link href="https://ojon.github.io/feeds/data-science.atom.xml" rel="self"></link><id>https://ojon.github.io/</id><updated>2018-08-11T16:00:00-03:00</updated><entry><title>Data leakage pitfalls in data science</title><link href="https://ojon.github.io/dataLeakage.html" rel="alternate"></link><published>2018-08-11T16:00:00-03:00</published><updated>2018-08-11T16:00:00-03:00</updated><author><name>João Oda</name></author><id>tag:ojon.github.io,2018-08-11:/dataLeakage.html</id><summary type="html">&lt;!-- Status: draft --&gt;&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;&lt;img alt="mario_see_data_leakage" src="/images/leakimg.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;In an attempt to organize my projects, I have just refactored an old &lt;a href="https://github.com/ojon/MD_Proj"&gt;data mining project&lt;/a&gt; done a few years ago. Originally I used Python for data preparation and feature selection and then with [Weka] (https://www.cs.waikato.ac.nz/ml/weka/) I performed the training, validation and model …&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</summary><content type="html">&lt;!-- Status: draft --&gt;&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;&lt;img alt="mario_see_data_leakage" src="/images/leakimg.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;In an attempt to organize my projects, I have just refactored an old &lt;a href="https://github.com/ojon/MD_Proj"&gt;data mining project&lt;/a&gt; done a few years ago. Originally I used Python for data preparation and feature selection and then with [Weka] (https://www.cs.waikato.ac.nz/ml/weka/) I performed the training, validation and model selection. I reimplemented everything in Python with [sckit-learn] (http://scikit-learn.org/stable/), making use of &lt;a href="http://scikit-learn.org/stable/modules/pipeline .html # pipeline"&gt;&lt;em&gt;pipelines&lt;/em&gt;&lt;/a&gt; and used the opportunity to fix a data leakage issue.&lt;/p&gt;
&lt;p&gt;In the context of data science, in a broad sense we have two cases of &lt;strong&gt;data leakage&lt;/strong&gt; in predictive models:&lt;/p&gt;
&lt;h2&gt;Leaking Features&lt;/h2&gt;
&lt;p&gt;It occurs when the set of training features has information (typically from the variable we want to predict) that will not be present when we perform the prediction in production environment.&lt;/p&gt;
&lt;h3&gt;Examples&lt;/h3&gt;
&lt;p&gt;Suppose you want to create a predictive model, which predicts whether a loan will be paid on time. Taking a real set of data as an example, on the &lt;a href="https://www.lendingclub.com/info/download-data.action"&gt;lendingclub website&lt;/a&gt; we have a database available, where one of the columns "total_rec_late_fee", informs the late fees received so far. If the value of this column is different from zero, it is clear that the loan was not paid on time. We can not use this information, which will only be available after the loan is granted, since a model that makes a prediction before making a loan is what is expected. In this same data source, there are many other columns to be disregarded for similar reason.&lt;/p&gt;
&lt;p&gt;In healthcare, a database can present a list of medicines that a person takes and you are trying to develop a model to make a diagnosis. You should take care to find out if the list in the database has been updated after the doctor's diagnosis due to a prescription of the doctor or if they were medicines that the person had already taken due to some condition they have.&lt;/p&gt;
&lt;p&gt;Now a case I dealt with in the past when working with trading algorithms. Imagine your data is a time series, where &lt;span class="math"&gt;\(x_{t-1}\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_{t+1}\)&lt;/span&gt; are known, but &lt;span class="math"&gt;\(x_t\)&lt;/span&gt; is a missing data. You decide, to work around this problem, do an interpolation where &lt;span class="math"&gt;\(x_{t} = \frac{x_{t-1} + x_{t+1}}{2}\)&lt;/span&gt; to fill the missing data. This approach compromises the creation of a model that uses data until time t to predict what happens t + 1, since there was a leakage of data from time t + 1 to the previous time t.&lt;/p&gt;
&lt;h2&gt;Leakage in Training Examples&lt;/h2&gt;
&lt;p&gt;It occurs when information from the validation set is used to train the model. You may think that simply partitioning your data set into training and testing before performing the training is enough. However the situation may be a little more complicated and the leakage can occur previously, during data preparation, selection of attributes, reduction of dimensionality and being a purist even a wide data visualization.&lt;/p&gt;
&lt;p&gt;Now I will illustrate a feature selection process that I used in the &lt;a href="https://github.com/ojon/MD_Proj"&gt;project&lt;/a&gt; mentioned at the beginning of this post. This project deals with data from &lt;a href="https://en.wikipedia.org/wiki/Gene_expression"&gt;gene expression&lt;/a&gt; (&lt;a href="https://en.wikipedia.org/wiki/DNA_microarray"&gt;&lt;em&gt;microarray&lt;/em&gt;&lt;/a&gt;) where the number of features is much larger than the number of samples. In this case using a very large number of attributes implies in a model which training takes a longer time, with less interpretability and more prone to overfitting.&lt;/p&gt;
&lt;h3&gt;Features selection using the t test&lt;/h3&gt;
&lt;p&gt;A simple way to perform feature selection is through a t-test based filter. In this version of the filter, for each label to be predicted, the filter bipartises the samples and performs a t-test. The top &lt;code&gt;w&lt;/code&gt; features, with the highest absolute t-value for each label, are selected&lt;/p&gt;
&lt;p&gt;I've implemented the filter in python as a &lt;em&gt;transformer&lt;/em&gt;, following the sckit-learn library standard. In order to do that we extend the classes &lt;code&gt;BaseEstimator&lt;/code&gt; and &lt;code&gt;TransformerMixin&lt;/code&gt; and implement the methods &lt;code&gt;fit&lt;/code&gt; and&lt;code&gt;transform&lt;/code&gt;. This makes it possible to use &lt;a href="http://scikit-learn.org/stable/modules/pipeline.html#pipeline"&gt;&lt;em&gt;pipelines&lt;/em&gt;&lt;/a&gt; from the same library. To apply the filter we call the two methods, &lt;code&gt;fit&lt;/code&gt; and&lt;code&gt;transform&lt;/code&gt;, in sequence.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;TtestScoreSelection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseEstimator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TransformerMixin&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_shape_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;

        &lt;span class="c1"&gt;# Check that X and y have correct shape&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;check_X_y&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Store the classes seen during fit&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unique_labels&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tValuesDF&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sortedIndexes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;sample1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;sample2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;st&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ttest_ind&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;equal_var&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tValuesDF&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;                      
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sortedIndexes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tValuesDF&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                                   &lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;

        &lt;span class="c1"&gt;# Return the transformer&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Check is fit had been called&lt;/span&gt;
        &lt;span class="n"&gt;check_is_fitted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'input_shape_'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="c1"&gt;# Input validation&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# union of indexes from the top w columns for each label&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;selCols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sortedIndexes&lt;/span&gt;&lt;span class="p"&gt;[:][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;selCols&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;

&lt;h3&gt;Predictions from random numbers&lt;/h3&gt;
&lt;p&gt;Let's now use our filter to select features in a wrong way. Consider the following didactic experiment:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Let's generate a random set of 100 samples of 100,000 Attributes.&lt;/li&gt;
&lt;li&gt;Reduce the number of features through the &lt;code&gt;TtestScoreSelection&lt;/code&gt; filter&lt;/li&gt;
&lt;li&gt;Train a SVM using cross-validation of 5-folds&lt;/li&gt;
&lt;/ol&gt;
&lt;table class="highlighttable"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;#random data generation&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;#feature selection&lt;/span&gt;
&lt;span class="n"&gt;tscoreSel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TtestScoreSelection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;tscoreSel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;selX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tscoreSel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;#SVM training and acuracy estimation by cross-validation.&lt;/span&gt;
&lt;span class="n"&gt;lr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;svm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SVC&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="n"&gt;scores_leak&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;scores_leak&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'score'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cross_val_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;selX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;As a result we have an accuracy close to 100% over all 5 folds.&lt;/p&gt;
&lt;p&gt;&lt;img alt="resultados_com_vazamento_de_dados" src="/images/scores_with_leak.png"/&gt;&lt;/p&gt;
&lt;p&gt;How is this possible ??? Make predictions with near 100% accuracy, from random numbers ?!&lt;/p&gt;
&lt;p&gt;If we carefully analyze our procedures we can observe that during step 2, the selection of variables occurred using the whole dataset. Since our filter makes use of the class labels to partition the samples and perform the test, in this step we indirectly propagate information from the data set that will be used in the future to validate the training set.&lt;/p&gt;
&lt;p&gt;The training depends on which features were selected and these were selected using even relevant information from the test set. So we have training with leaked examples.&lt;/p&gt;
&lt;h3&gt;Fixing data lekage&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Mario" src="/images/mario_wanna_fix_leakage.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;Someone tells Mario that to fix this data leak that "allowed" a prediction from random numbers we do not need a pipe wrench, but use the correct methodology and write good code. Therefore, we need to separate a dataset, without any intersection with the validation set, where feature selection and training occurs. As we are performing cross-validation of 5-folds, this will be repeated 5 times, the total data set will be partitioned into 5 folds and each time a fold will be used for validation and the complement for feature selection and training.&lt;/p&gt;
&lt;p&gt;Scikit-learn allows us to use the [&lt;em&gt;pipelines&lt;/em&gt;] feature (http://scikit-learn.org/stable/modules/pipeline.html#pipeline) to implement the processing steps that the data is submitted. The &lt;code&gt;cross_val_score ()&lt;/code&gt; function is capable of receiving a pipeline as an argument and performing training and cross-validation in k folds. No wonder I implemented the filter in a way compatible with pipelines. So we have the following code:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5
6
7&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;pipe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'featureSelection'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TtestScoreSelection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'classify'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;svm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SVC&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'score'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cross_val_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pipe&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;The results are:&lt;/p&gt;
&lt;p&gt;&lt;img alt="resultados_sem_vazamento_de_dados" src="/images/scores_without_leak.png"/&gt;&lt;/p&gt;
&lt;p&gt;As expected, after all, one can expect nothing more than an accuracy of around 50% for a binary classification from random data.&lt;/p&gt;
&lt;h2&gt;Final considerations&lt;/h2&gt;
&lt;p&gt;Data leaks can lead to unpleasant surprises, such as models that perform better in a development environment than in production. Always be careful, because the data leakage is not always explicitly exposed.&lt;/p&gt;
&lt;p&gt;A similar post dealing with leaked training with an R approach can be found on &lt;a href="https://johanndejong.wordpress.com/2017/08/06/feature-selection-cross-validation-and -data-leakage /"&gt;Johann de Jong's blog&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;</content><category term="data science"></category><category term="intermediate"></category><category term="AI"></category><category term="ML"></category><category term="feature selection"></category><category term="validation"></category><category term="data mining"></category><category term="scikit-learn"></category></entry><entry><title>Introduction to Data Science with a Mnemonic Image</title><link href="https://ojon.github.io/aboutDataScience.html" rel="alternate"></link><published>2018-07-23T16:00:00-03:00</published><updated>2018-07-23T16:00:00-03:00</updated><author><name>João Oda</name></author><id>tag:ojon.github.io,2018-07-23:/aboutDataScience.html</id><summary type="html">&lt;!-- Summary: Whats is Data Science about. --&gt;&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;Another day I found on the web this curious drawing (clearly inspired on &lt;a href="https://en.wikipedia.org/wiki/Mega_Man"&gt;megaman&lt;/a&gt;).&lt;/p&gt;
&lt;figure width="600"&gt;
  &lt;img alt="DiceMan" src="https://pre00.deviantart.net/c0d7/th/pre/i/2010/134/e/5/rpn_005_dice_man_by_jecht_striker.jpg"/&gt;
  &lt;figcaption style="text-align:right"&gt;drawing credits to VR-Hyoumaru&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Let's take a look at this picture and do some associations.&lt;/p&gt;
&lt;p&gt;Dices are one the main symbols of &lt;strong&gt;probability theory&lt;/strong&gt;, a field of &lt;strong&gt;mathematics&lt;/strong&gt; that with many applications and that provides the …&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</summary><content type="html">&lt;!-- Summary: Whats is Data Science about. --&gt;&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;Another day I found on the web this curious drawing (clearly inspired on &lt;a href="https://en.wikipedia.org/wiki/Mega_Man"&gt;megaman&lt;/a&gt;).&lt;/p&gt;
&lt;figure width="600"&gt;
  &lt;img alt="DiceMan" src="https://pre00.deviantart.net/c0d7/th/pre/i/2010/134/e/5/rpn_005_dice_man_by_jecht_striker.jpg"/&gt;
  &lt;figcaption style="text-align:right"&gt;drawing credits to VR-Hyoumaru&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Let's take a look at this picture and do some associations.&lt;/p&gt;
&lt;p&gt;Dices are one the main symbols of &lt;strong&gt;probability theory&lt;/strong&gt;, a field of &lt;strong&gt;mathematics&lt;/strong&gt; that with many applications and that provides the foundations to &lt;strong&gt;statistics&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Here I will associate the robot with &lt;strong&gt;automation&lt;/strong&gt; and &lt;strong&gt;computation&lt;/strong&gt; (although other fields like mechatronics might come to your mind).&lt;/p&gt;
&lt;p&gt;Let me add some extra piece of information. In Portuguese the word "dados" can mean &lt;strong&gt;data&lt;/strong&gt;, but it can also means dices.&lt;/p&gt;
&lt;p&gt;Now imagine whatever our "data/dice man" mission is, it ll be in an &lt;strong&gt;environment&lt;/strong&gt;, a &lt;strong&gt;context&lt;/strong&gt; and the more he knows about it, better will be outcome.&lt;/p&gt;
&lt;p&gt;I am about to introduce the field of &lt;strong&gt;Data Science&lt;/strong&gt; and maybe these associations and a mental picture will help you memorize the concept, a trick I find useful in many situations.&lt;/p&gt;
&lt;h2&gt;What's Data Science About?&lt;/h2&gt;
&lt;p&gt;In recent years the term &lt;strong&gt;Data Science&lt;/strong&gt; has emerged to nominate a interdisciplinary area of knowledge. Data scientists and other professionals of this area provides organizations with some form of analytics.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;descriptive analytics&lt;/strong&gt; can show some insights and point out hidden information, you need to analyze these and see how they can affect your business decisions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;predictive analytics&lt;/strong&gt; uses models to predict an outcome that extrapolates the information we have until now, in many cases is predict the future.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On &lt;strong&gt;prescriptive analytics&lt;/strong&gt; you careful select your optimization metric and you will use data to decide what actions take to maximize your metric.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Summary for me Data Science is about &lt;strong&gt;obtain value from data&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;Skills&lt;/h2&gt;
&lt;p&gt;Data science is closely related with &lt;em&gt;Big Data&lt;/em&gt; phenomena. To deal with massive amounts of data we need to employ computational tools guided by statistical and math principles with in a business context.&lt;/p&gt;
&lt;p&gt;Now I will borrow from &lt;a href="http://berkeleysciencereview.com/how-to-become-a-data-scientist-before-you-graduate/"&gt;berkeleysciencereview&lt;/a&gt; (right now, working on item 9 from the link) a more common info-graphic that summarizes this idea.&lt;/p&gt;
&lt;p&gt;&lt;img alt="infograph" src="http://i.imgur.com/aoz1BJy.jpg"/&gt;&lt;/p&gt;
&lt;h2&gt;In order to be effective, be careful!&lt;/h2&gt;
&lt;p&gt;You must know the rules of the game that you are playing. In Data Science you need &lt;strong&gt;domain expertise&lt;/strong&gt;. You don't need to be an expert in the field but at least communicate with the other professionals that will help you with insights and what is relevant.&lt;/p&gt;
&lt;p&gt;You come up with a list of questions, you might attempt answer with data, but before put &lt;em&gt;analytics&lt;/em&gt; effort on it, it is wise to find out what questions, if answered, &lt;strong&gt;add more value to the business&lt;/strong&gt;. Maybe it's the case you don't even have the data yet. Does it worth enhance the accuracy of a model by 0.1% or better move to another questions. Is &lt;em&gt;accuracy&lt;/em&gt; the best way to evaluate the answer to your problem?&lt;/p&gt;
&lt;p&gt;On &lt;em&gt;prescriptive analytics&lt;/em&gt; you need careful select your optimization metric in order to &lt;strong&gt;match your values&lt;/strong&gt;, otherwise you may end in an undesired point.&lt;/p&gt;
&lt;p&gt;You need to &lt;strong&gt;know you data&lt;/strong&gt;, the threats to your analysis and limitations. As unfair coins and dices, you data might have bias.&lt;/p&gt;
&lt;h2&gt;Other perspectives&lt;/h2&gt;
&lt;p&gt;As a field that is still maturing, you will come across multiple definitions. I will leave you now with some from Columbia professors:&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
  &lt;iframe allow="autoplay; encrypted-media" allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/a8abc9SgVUM" width="560"&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;I borrow this video from edX MOOC I have done "DS101X: Statistical Thinking for Data Science and Analytics"&lt;/p&gt;
&lt;h2&gt;Final considerations&lt;/h2&gt;
&lt;p&gt;I have interest in AI in general and one of the main things I like about Data Science is that it brings AI, especially machine learning to daily practical use. Most of times it will be in a pervasive way, but it's there.   &lt;/p&gt;
&lt;p&gt;I hope now you may have a better idea of one of the main topics of this blog.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</content><category term="data science"></category><category term="intro"></category><category term="beginner"></category><category term="AI"></category><category term="ML"></category></entry></feed>